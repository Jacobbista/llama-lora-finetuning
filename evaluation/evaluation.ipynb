{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d18eaa8",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f700203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm as notebook_tqdm\n",
    "from datasets import load_dataset\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "\n",
    "import multiprocessing\n",
    "n_threads = multiprocessing.cpu_count()  # use all CPU cores\n",
    "\n",
    "JUDGE_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "openai_client = OpenAI()  # reads OPENAI_API_KEY from env\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    \"base_1b\": {\n",
    "        \"local_path\": \"\",\n",
    "        \"hf_repo\": \"unsloth/Llama-3.2-1B-Instruct-GGUF\",\n",
    "        \"hf_file\": \"Llama-3.2-1B-Instruct-Q4_K_M.gguf\",\n",
    "        \"revision\": None,\n",
    "    },\n",
    "    \"base_3b\": {\n",
    "        \"local_path\": \"\",\n",
    "        \"hf_repo\": \"unsloth/Llama-3.2-3B-Instruct-GGUF\",\n",
    "        \"hf_file\": \"Llama-3.2-3B-Instruct-Q4_K_M.gguf\",\n",
    "        \"revision\": None,\n",
    "    },\n",
    "    \"1b_qlora\": {\n",
    "        \"local_path\": \"..models/gguf_1B_QLORA/Llama-3.2-1B-Instruct.Q4_K_M.gguf\",\n",
    "        \"hf_repo\": \"jacobbista/llama3-1b-finetome\",\n",
    "        \"hf_file\": \"Llama-3.2-1B-Instruct.Q4_K_M.gguf\",\n",
    "        \"revision\": \"1B_QLoRA_N1000\",\n",
    "    },\n",
    "    \"1b_lora\": {\n",
    "        \"local_path\": \"..models/gguf_1B_LORA/Llama-3.2-1B-Instruct.Q4_K_M.gguf\",\n",
    "        \"hf_repo\": \"jacobbista/llama3-1b-finetome\",\n",
    "        \"hf_file\": \"Llama-3.2-1B-Instruct.Q4_K_M.gguf\",\n",
    "        \"revision\": \"1B_LoRA_N1000\",\n",
    "    },\n",
    "    \"3b_qlora\": {\n",
    "        \"local_path\": \"..models/gguf_3B_QLORA/Llama-3.2-3B-Instruct.Q4_K_M.gguf\",\n",
    "        \"hf_repo\": \"jacobbista/llama3-3b-finetome\",\n",
    "        \"hf_file\": \"Llama-3.2-3B-Instruct.Q4_K_M.gguf\",\n",
    "        \"revision\": \"3B_QLoRA_N1000\",\n",
    "    },\n",
    "}\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "MAX_NEW_TOKENS = 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64c7791",
   "metadata": {},
   "source": [
    "## Build Eval set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "49b0737e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,\n",
       " {'id': 'E0',\n",
       "  'instruction': 'How do astronomers measure the distance to stars within our galaxy?',\n",
       "  'reference': 'Stellar parallax is a geometric method that relies on the principle of triangula...'})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carica FineTome\n",
    "raw = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")\n",
    "\n",
    "# We used the first 1000 samples for training\n",
    "# Take 20 examples immediately after\n",
    "EVAL_START = 1000\n",
    "EVAL_SIZE = 20\n",
    "\n",
    "eval_raw = raw.select(range(EVAL_START, EVAL_START + EVAL_SIZE))\n",
    "\n",
    "eval_examples = []\n",
    "for idx, convo in enumerate(eval_raw[\"conversations\"]):\n",
    "    try:\n",
    "        user_msg = convo[0][\"value\"]\n",
    "        asst_msg = convo[1][\"value\"]\n",
    "    except (IndexError, KeyError):\n",
    "        continue\n",
    "\n",
    "    eval_examples.append({\n",
    "        \"id\": f\"E{idx}\",\n",
    "        \"instruction\": user_msg,\n",
    "        \"reference\": asst_msg,\n",
    "    })\n",
    "\n",
    "trunc = lambda d, n=80: {k: (v[:n]+'...' if isinstance(v, str) and len(v)>n else v) for k,v in d.items()}\n",
    "\n",
    "len(eval_examples), trunc(eval_examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76571c64",
   "metadata": {},
   "source": [
    "## Helper functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ef8c04",
   "metadata": {},
   "source": [
    "### Load gguf model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d7bb26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gguf_model(model_key: str) -> Llama:\n",
    "    cfg = MODEL_CONFIGS[model_key]\n",
    "\n",
    "    # Test if local path exists\n",
    "    if os.path.exists(cfg[\"local_path\"]):\n",
    "        model_path = cfg[\"local_path\"]\n",
    "        print(f\"[{model_key}] âœ“ Using local GGUF: {model_path}\")\n",
    "    else:\n",
    "        # Fallback to HF\n",
    "        print(f\"[{model_key}] â†ª Local GGUF not found, downloading from HF...\")\n",
    "        model_path = hf_hub_download(\n",
    "            repo_id  = cfg[\"hf_repo\"],\n",
    "            filename = cfg[\"hf_file\"],\n",
    "            revision = cfg[\"revision\"],\n",
    "            # tqdm_class=notebook_tqdm, # not present in v0.36.0\n",
    "        )\n",
    "        print(f\"[{model_key}] âœ“ Downloaded from HF: {model_path}\")\n",
    "    \n",
    "    llm = Llama(\n",
    "        model_path=model_path,\n",
    "        n_ctx=MAX_SEQ_LENGTH,\n",
    "        n_gpu_layers=0,            # CPU-only\n",
    "        n_threads=n_threads,       # threads for generation\n",
    "        n_threads_batch=n_threads, # threads for prompt processing\n",
    "        verbose=False,\n",
    "        seed=47,\n",
    "    )\n",
    "    print(f\"[{model_key}] Using {n_threads} CPU threads\")\n",
    "    return llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e848e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PERSONA_PROMPT = \"You are a helpful AI assistant.\"\n",
    "\n",
    "def build_prompt(instruction: str, persona_prompt: str = DEFAULT_PERSONA_PROMPT) -> str:\n",
    "    prompt_str = \"\"\n",
    "    prompt_str += f\"<|start_header_id|>system<|end_header_id|>\\n\\n{persona_prompt}<|eot_id|>\"\n",
    "    prompt_str += f\"<|start_header_id|>user<|end_header_id|>\\n\\n{instruction}<|eot_id|>\"\n",
    "    prompt_str += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    return prompt_str\n",
    "\n",
    "\n",
    "def generate_answer(llm: Llama, instruction: str, max_new_tokens: int = MAX_NEW_TOKENS) -> str:\n",
    "    prompt_str = build_prompt(instruction)\n",
    "    out = llm(\n",
    "        prompt_str,\n",
    "        max_tokens=max_new_tokens,\n",
    "        stop=[\"<|eot_id|>\"],\n",
    "        temperature=0.0,  # determinsm\n",
    "        top_p=0.9,\n",
    "        echo=False,\n",
    "    )\n",
    "    # llama_cpp in this mode returns 'choices' with 'text'\n",
    "    return out[\"choices\"][0][\"text\"].strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1bc97a",
   "metadata": {},
   "source": [
    "## LLM-as-a-judge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "38c3c259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_answer(instruction: str, reference: str, candidate: str) -> dict:\n",
    "    system_msg = (\n",
    "        \"You are an expert evaluator for instruction-following models. \"\n",
    "        \"Given an instruction, a reference answer, and a candidate answer, \"\n",
    "        \"you must output a JSON object with two fields:\\n\"\n",
    "        '{ \"score\": <int 1-5>, \"explanation\": \"<brief reason>\" }.\\n'\n",
    "        \"Score meanings:\\n\"\n",
    "        \"1 = totally wrong or irrelevant,\\n\"\n",
    "        \"3 = partially correct but incomplete or sloppy,\\n\"\n",
    "        \"5 = very close to or better than the reference.\"\n",
    "    )\n",
    "\n",
    "    user_msg = f\"\"\"\n",
    "        Instruction:\n",
    "        {instruction}\n",
    "\n",
    "        Reference answer:\n",
    "        {reference}\n",
    "\n",
    "        Candidate answer:\n",
    "        {candidate}\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=JUDGE_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "\n",
    "    return json.loads(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23da0d29",
   "metadata": {},
   "source": [
    "## Evalutation loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9f765d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Evaluating model: base_1b ...\n",
      "[base_1b] â†ª Local GGUF not found, downloading from HF...\n",
      "[base_1b] âœ“ Downloaded from HF: /home/jacobbista/.cache/huggingface/hub/models--unsloth--Llama-3.2-1B-Instruct-GGUF/snapshots/b69aef112e9f895e6f98d7ae0949f72ff09aa401/Llama-3.2-1B-Instruct-Q4_K_M.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[base_1b] Using 22 CPU threads\n",
      "  âœ“ Finished base_1b â€” Avg Score: 3.60\n",
      "\n",
      "ðŸš€ Evaluating model: base_3b ...\n",
      "[base_3b] â†ª Local GGUF not found, downloading from HF...\n",
      "[base_3b] âœ“ Downloaded from HF: /home/jacobbista/.cache/huggingface/hub/models--unsloth--Llama-3.2-3B-Instruct-GGUF/snapshots/e7d0997e49c9cb00d88b4c1a6a16aa894b0bbc31/Llama-3.2-3B-Instruct-Q4_K_M.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[base_3b] Using 22 CPU threads\n",
      "  âœ“ Finished base_3b â€” Avg Score: 3.60\n",
      "\n",
      "ðŸš€ Evaluating model: 1b_qlora ...\n",
      "[1b_qlora] â†ª Local GGUF not found, downloading from HF...\n",
      "[1b_qlora] âœ“ Downloaded from HF: /home/jacobbista/.cache/huggingface/hub/models--jacobbista--llama3-1b-finetome/snapshots/64aabacdf31fb5bb5cbb7fd3a7b5960c8ceed172/Llama-3.2-1B-Instruct.Q4_K_M.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1b_qlora] Using 22 CPU threads\n",
      "  âœ“ Finished 1b_qlora â€” Avg Score: 3.35\n",
      "\n",
      "ðŸš€ Evaluating model: 1b_lora ...\n",
      "[1b_lora] â†ª Local GGUF not found, downloading from HF...\n",
      "[1b_lora] âœ“ Downloaded from HF: /home/jacobbista/.cache/huggingface/hub/models--jacobbista--llama3-1b-finetome/snapshots/2abda01f62312cc4de50aa0a7d00ee42fa4deaad/Llama-3.2-1B-Instruct.Q4_K_M.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1b_lora] Using 22 CPU threads\n",
      "  âœ“ Finished 1b_lora â€” Avg Score: 3.15\n",
      "\n",
      "ðŸš€ Evaluating model: 3b_qlora ...\n",
      "[3b_qlora] â†ª Local GGUF not found, downloading from HF...\n",
      "[3b_qlora] âœ“ Downloaded from HF: /home/jacobbista/.cache/huggingface/hub/models--jacobbista--llama3-3b-finetome/snapshots/bf32d407ffa24299ba965ece4fdb2e1cb785daad/Llama-3.2-3B-Instruct.Q4_K_M.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3b_qlora] Using 22 CPU threads\n",
      "  âœ“ Finished 3b_qlora â€” Avg Score: 3.65\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "TOTAL_EX = len(eval_examples)\n",
    "\n",
    "for model_key in MODEL_CONFIGS:\n",
    "    print(f\"\\nðŸš€ Evaluating model: {model_key} ...\")\n",
    "    llm = load_gguf_model(model_key)\n",
    "    \n",
    "    scores = []\n",
    "\n",
    "    for i, ex in enumerate(eval_examples, start=1):\n",
    "        instr = ex[\"instruction\"]\n",
    "        ref   = ex[\"reference\"]\n",
    "\n",
    "        print(f\"  â†’ [{i}/{TOTAL_EX}] Testing sample {ex['id']} ...\", end=\"\\r\")\n",
    "\n",
    "        cand = generate_answer(llm, instr)\n",
    "        judge = judge_answer(instr, ref, cand)\n",
    "\n",
    "        score = judge.get(\"score\", 0)\n",
    "        scores.append(score)\n",
    "\n",
    "        results.append({\n",
    "            \"model\": model_key,\n",
    "            \"id\": ex[\"id\"],\n",
    "            \"instruction\": instr,\n",
    "            \"reference\": ref,\n",
    "            \"candidate\": cand,\n",
    "            \"score\": score,\n",
    "            \"explanation\": judge.get(\"explanation\", \"\"),\n",
    "        })\n",
    "\n",
    "    avg_score = sum(scores) / len(scores)\n",
    "    print(f\"  âœ“ Finished {model_key} â€” Avg Score: {avg_score:.2f}\")\n",
    "\n",
    "    del llm  # free RAM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829efa68",
   "metadata": {},
   "source": [
    "### Save Locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "88a4b119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>id</th>\n",
       "      <th>instruction</th>\n",
       "      <th>reference</th>\n",
       "      <th>candidate</th>\n",
       "      <th>score</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>base_1b</td>\n",
       "      <td>E0</td>\n",
       "      <td>How do astronomers measure the distance to sta...</td>\n",
       "      <td>Stellar parallax is a geometric method that re...</td>\n",
       "      <td>Measuring the distance to stars within our gal...</td>\n",
       "      <td>4</td>\n",
       "      <td>The candidate answer provides a good overview ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>base_1b</td>\n",
       "      <td>E1</td>\n",
       "      <td>Write a program in Python to determine if a gi...</td>\n",
       "      <td>Here's a program in Python that checks if a gi...</td>\n",
       "      <td>**Palindrome Checker Program**\\n==============...</td>\n",
       "      <td>3</td>\n",
       "      <td>The candidate answer uses a two-pointer techni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>base_1b</td>\n",
       "      <td>E2</td>\n",
       "      <td>How can I write a Python program to find the n...</td>\n",
       "      <td>You can achieve this by following these steps ...</td>\n",
       "      <td>You can use the `count()` method in Python to ...</td>\n",
       "      <td>5</td>\n",
       "      <td>The candidate answer provides a clear and effi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>base_1b</td>\n",
       "      <td>E3</td>\n",
       "      <td>How can I reverse a string using Python?</td>\n",
       "      <td>To reverse a given string in Python, you can u...</td>\n",
       "      <td>You can reverse a string in Python using the `...</td>\n",
       "      <td>5</td>\n",
       "      <td>The candidate answer provides a correct and va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>base_1b</td>\n",
       "      <td>E4</td>\n",
       "      <td>Write Python code to solve the task:\\nLittle P...</td>\n",
       "      <td>Step 1:  The given code contains several unrel...</td>\n",
       "      <td>Here is a Python solution for the problem:\\n\\n...</td>\n",
       "      <td>5</td>\n",
       "      <td>The candidate answer correctly implements the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model  id                                        instruction  \\\n",
       "0  base_1b  E0  How do astronomers measure the distance to sta...   \n",
       "1  base_1b  E1  Write a program in Python to determine if a gi...   \n",
       "2  base_1b  E2  How can I write a Python program to find the n...   \n",
       "3  base_1b  E3           How can I reverse a string using Python?   \n",
       "4  base_1b  E4  Write Python code to solve the task:\\nLittle P...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Stellar parallax is a geometric method that re...   \n",
       "1  Here's a program in Python that checks if a gi...   \n",
       "2  You can achieve this by following these steps ...   \n",
       "3  To reverse a given string in Python, you can u...   \n",
       "4  Step 1:  The given code contains several unrel...   \n",
       "\n",
       "                                           candidate  score  \\\n",
       "0  Measuring the distance to stars within our gal...      4   \n",
       "1  **Palindrome Checker Program**\\n==============...      3   \n",
       "2  You can use the `count()` method in Python to ...      5   \n",
       "3  You can reverse a string in Python using the `...      5   \n",
       "4  Here is a Python solution for the problem:\\n\\n...      5   \n",
       "\n",
       "                                         explanation  \n",
       "0  The candidate answer provides a good overview ...  \n",
       "1  The candidate answer uses a two-pointer techni...  \n",
       "2  The candidate answer provides a clear and effi...  \n",
       "3  The candidate answer provides a correct and va...  \n",
       "4  The candidate answer correctly implements the ...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"evaluation_results.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021b4704",
   "metadata": {},
   "source": [
    "## Data aggregation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "446439e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model\n",
       "3b_qlora    3.65\n",
       "base_3b     3.60\n",
       "base_1b     3.60\n",
       "1b_qlora    3.35\n",
       "1b_lora     3.15\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"model\")[\"score\"].mean().sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab2-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
